{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute statistical results of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to load results obtain from the multi graph matching algorithms coded in matlab (that were given previously simulated graphs) and to compute statistics about these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the folder\n",
    "path = \"test_for_pairwise/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the data in a 3 dimensional tensor where the first dimension correspond to the noise parameter, the second to the outliers parameter and the third one to the different runs for a given set of parameters. First we need to give to each possible value of a parameter an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first go through all files to determine the parameters that were\n",
    "# chosen and assign an integer to them\n",
    "\n",
    "def get_noise_and_outliers_params(path_to_folder, number_parameters=2):\n",
    "    \"\"\" go through all the folders to determine the possible\n",
    "        parameters and assign them an integer value\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for folder_name in os.listdir(path_to_folder):\n",
    "        list_splitted = folder_name.split(\",\")\n",
    "        \n",
    "        for param_num, elem_param in enumerate(list_splitted):\n",
    "            param_name, param_value = elem_param.split(\"_\")\n",
    "            param_value = float(param_value)\n",
    "            \n",
    "            # Create the parameter entries in the result\n",
    "            if param_name not in result:\n",
    "                result[param_name] = []\n",
    "                \n",
    "            if param_value not in result[param_name]: \n",
    "                result[param_name].append(param_value)\n",
    "            \n",
    "    # We sort the dict for later plotting purposes\n",
    "    for param_name in result:\n",
    "        result[param_name].sort()\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "#dict_parameters_correspondence = get_noise_and_outliers_params(path)\n",
    "#print(dict_parameters_correspondence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MES TESTS D'ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de fonctions d'accuracy\n",
    "path_run_folder = os.path.join(path, \"noise_1000,outliers_4\",\"0\")\n",
    "ground_truth = np.load(os.path.join(path_run_folder,\"ground_truth.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_run_folder_1000 = os.path.join(path, \"noise_1000,outliers_0\",\"0\")\n",
    "# ground_truth_1000 = np.load(os.path.join(path_run_folder_1000,\"ground_truth.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pairwise\n",
    "# print(\"gt shape :\", ground_truth.shape) # nb_graphs, nb_graphs, nb_vertices\n",
    "\n",
    "# name_of_result_file = \"X_cao_cst_o\"\n",
    "# algorithm_res = sio.loadmat(os.path.join(path_run_folder,name_of_result_file+\".mat\"))[\"X\"]\n",
    "# print(\"algo res shape :\",algorithm_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multigraph\n",
    "# print(\"gt shape :\", ground_truth.shape) # nb_graphs, nb_graphs, nb_vertices\n",
    "\n",
    "# name_of_result_file = \"A_mALS\"\n",
    "# algorithm_res = sio.loadmat(os.path.join(path_run_folder,name_of_result_file+\".mat\"))[\"A\"]\n",
    "# algorithm_res = algorithm_res@algorithm_res.T\n",
    "\n",
    "# algorithm_res = np.where(algorithm_res > 0.3, 1, 0)\n",
    "\n",
    "# print(\"algo res shape :\",algorithm_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm_res= np.load(os.path.join(path_run_folder+\"/matchALS_result_new.npy\"))\n",
    "# algorithm_res.shape\n",
    "# algorithm_res = np.where(algorithm_res > 0.45, 1, 0)\n",
    "# algorithm_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hippi_result = np.load(\"Hippi_res_mat.npy\")\n",
    "gt = np.load(\"ground_truth.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hippi_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_permutation(target_permutation, given_permutation):\n",
    "    \"\"\" Return the accuracy of a given permutation knowing the ground truth permutation\n",
    "        - ground_truth_permutation is a np array where the first axis corresponds to one graph of the family\n",
    "        the second axis corresponds to another graph and the third axis contains the\n",
    "        ground truth correspondence matrix between the first and second graph\n",
    "        - given_permutation is a bulk matrix where all the permutation marix of the algorithm\n",
    "        are put together.\n",
    "    \"\"\"\n",
    "    \n",
    "    count_ok = 0 # match\n",
    "    count_not_ok = 0 # mismatch\n",
    "    count_total = 0\n",
    "    \n",
    "    nb_graphs = target_permutation.shape[0]\n",
    "    nb_nodes = int(given_permutation.shape[0] / nb_graphs)\n",
    "    \n",
    "    for graph_1 in range(target_permutation.shape[0]):\n",
    "        for graph_2 in range(target_permutation.shape[0]):\n",
    "            \n",
    "            # If we are looking at two different graphs we compute the accuracy\n",
    "            if graph_1 != graph_2:\n",
    "                \n",
    "                ground_truth_list = target_permutation[graph_1, graph_2, :]\n",
    "                sub_permutation = given_permutation[graph_1 * nb_nodes: (graph_1 + 1) * nb_nodes, graph_2 * nb_nodes: (graph_2 + 1) * nb_nodes]\n",
    "\n",
    "                for node_1, node_2 in enumerate(ground_truth_list):\n",
    "                    if sub_permutation[node_1, node_2] == 1:\n",
    "                        count_ok += 1\n",
    "                    elif sub_permutation[node_1, node_2] == 0:\n",
    "                        count_not_ok+=1\n",
    "                        \n",
    "                    count_total += 1\n",
    "                    \n",
    "    print(\"Number of Mismatch: \",count_not_ok)\n",
    "    print(count_total)\n",
    "    print(count_ok)\n",
    "                    \n",
    "    return count_ok/count_total\n",
    "                \n",
    "                \n",
    "#get_accuracy_permutation(ground_truth, algorithm_res)\n",
    "get_accuracy_permutation(gt, hippi_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_accuracy_permutation(gt,Qmatch_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_accuracy_permutation(gt,Qmatch_pyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm_res[0*30:2*30,1*30:2*30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = [100,400,1000]\n",
    "acc = [0.42,0.88,0.98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(noise,acc,linestyle='--', marker='o', color='black',label = \"Noise vs accuracy\")\n",
    "#plt.plot(outliers,noise_200,linestyle='--', marker='o', color='r',label = \"noise 200\")\n",
    "plt.xlabel(\"Noise\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers = [0,4,8,12,16]\n",
    "# noise_1000 = [0.98,0.65,0.36,0.34,0.19]\n",
    "# noise_200 = [0.88,0.33,0.4,0.13,0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(outliers,noise_1000,linestyle='--', marker='o', color='b',label = \"noise 1000\")\n",
    "plt.plot(outliers,noise_200,linestyle='--', marker='o', color='r',label = \"noise 200\")\n",
    "plt.xlabel(\"Outliers\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN DES TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to go through all the folder and build the 3D tensors that will hold the result. But first we need to define an accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_permutation(target_permutation, given_permutation):\n",
    "    \"\"\" Return the accuracy of a given permutation knowing the ground truth permutation\n",
    "        - ground_truth_permutation is a np array where the first axis corresponds to one graph of the family\n",
    "        the second axis corresponds to another graph and the third axis contains the\n",
    "        ground truth correspondence matrix between the first and second graph\n",
    "        - given_permutation is a bulk matrix where all the permutation marix of the algorithm\n",
    "        are put together.\n",
    "    \"\"\"\n",
    "    \n",
    "    count_ok = 0\n",
    "    count_total = 0\n",
    "    \n",
    "    nb_graphs = target_permutation.shape[0]\n",
    "    nb_nodes = int(given_permutation.shape[0] / nb_graphs)\n",
    "    \n",
    "    for graph_1 in range(target_permutation.shape[0]):\n",
    "        for graph_2 in range(target_permutation.shape[0]):\n",
    "            \n",
    "            # If we are looking at two different graphs we compute the accuracy\n",
    "            if graph_1 != graph_2:\n",
    "                \n",
    "                ground_truth_list = target_permutation[graph_1, graph_2, :]\n",
    "                sub_permutation = given_permutation[graph_1 * nb_nodes: (graph_1 + 1) * nb_nodes, graph_2 * nb_nodes: (graph_2 + 1) * nb_nodes]\n",
    "\n",
    "                for node_1, node_2 in enumerate(ground_truth_list):\n",
    "                    if sub_permutation[node_1, node_2] == 1:\n",
    "                        count_ok += 1\n",
    "                    count_total += 1\n",
    "                    \n",
    "    return count_ok/count_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_tensor_for_given_algorithm(path_to_folder, param_correspondence_dict, name_of_result_file=\"X_kergm\", transposed=False):\n",
    "    \"\"\"Go through all folders and build a 3D tensor that holds\n",
    "    the accuracy metric for all set of parameters for a given algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        path_to_folder: The path where the results have been calculated\n",
    "        param_correspondence_dict: The result of the get_noise_and_outliers_params\n",
    "    function that give an integer correspondence to each parameter value\n",
    "        name_of_result_file: The file name to load in each folder (each one\n",
    "    corresponds to a given algorithm)\n",
    "    \"\"\"\n",
    "    \n",
    "    # We need to find how many runs were used so we go though the first folder\n",
    "    nbRuns = len(os.listdir(os.path.join(path_to_folder, os.listdir(path_to_folder)[0])))\n",
    "    \n",
    "    # We initialise the final tensor that hold the results\n",
    "    result_tensor_shape = [len(param_correspondence_dict[param_name]) for param_name in param_correspondence_dict]\\\n",
    "                          + [nbRuns]\n",
    "    result_tensor = np.zeros(result_tensor_shape)\n",
    "    \n",
    "    #We go through all folders\n",
    "    for parameter_folder in os.listdir(path_to_folder):\n",
    "        \n",
    "        # define the new path\n",
    "        path_parameter_folder = os.path.join(path_to_folder, parameter_folder)\n",
    "        \n",
    "        # get the parameters\n",
    "        splitted_param = parameter_folder.split(\",\")\n",
    "        param_1_name, param_1_value = splitted_param[0].split(\"_\")\n",
    "        param_1_value = float(param_1_value)\n",
    "        param_1_integer = param_correspondence_dict[param_1_name].index(param_1_value)\n",
    "        \n",
    "        param_2_name, param_2_value = splitted_param[1].split(\"_\")\n",
    "        param_2_value = float(param_2_value)\n",
    "        param_2_integer = param_correspondence_dict[param_2_name].index(param_2_value)\n",
    "        \n",
    "        #print(param_1_name, param_1_value, param_2_name, param_2_value, param_1_integer, param_2_integer)\n",
    "        \n",
    "        # We go through all the runs\n",
    "        for run_i, run_folder in enumerate(os.listdir(path_parameter_folder)):\n",
    "            \n",
    "            path_run_folder = os.path.join(path_parameter_folder, run_folder)\n",
    "            \n",
    "            # load the ground truth corespondence\n",
    "            ground_truth = np.load(os.path.join(path_run_folder,\"ground_truth.npy\"))\n",
    "            \n",
    "            # load the algorithm result\n",
    "            algorithm_res = sio.loadmat(os.path.join(path_run_folder,name_of_result_file+\".mat\"))[\"X\"]\n",
    "            \n",
    "            # transpose it if necessary\n",
    "            if transposed:\n",
    "                algorithm_res = np.transpose(algorithm_res)\n",
    "            \n",
    "            # get the accuracy result\n",
    "            accuracy = get_accuracy_permutation(ground_truth, algorithm_res)\n",
    "            \n",
    "            # add the result to the tensor\n",
    "            result_tensor[param_1_integer, param_2_integer, run_i] = accuracy\n",
    "            \n",
    "    return result_tensor\n",
    "    \n",
    "#kergm_res = get_result_tensor_for_given_algorithm(path, res)\n",
    "#kergm_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coose the path to the result\n",
    "path = \"generated_graphs_medium/\"\n",
    "name_dataset = \"medium\"\n",
    "\n",
    "#load the dictionary of parameters integer correspondence\n",
    "dict_parameters_correspondence = get_noise_and_outliers_params(path)\n",
    "\n",
    "# compute the result of all the given algorithms\n",
    "transposed = True\n",
    "kergm_res = get_result_tensor_for_given_algorithm(path, dict_parameters_correspondence, \"X_kergm\", False)\n",
    "# ipf_res = get_result_tensor_for_given_algorithm(path, dict_parameters_correspondence, \"X_ipf\", transposed)\n",
    "# rrwm_res = get_result_tensor_for_given_algorithm(path, dict_parameters_correspondence, \"X_rrwm\", transposed)\n",
    "# smac_res = get_result_tensor_for_given_algorithm(path, dict_parameters_correspondence, \"X_smac\", transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_parameters_correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"results/medium\"\n",
    "\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "\n",
    "# Affect all the necesary variables\n",
    "dict_parameters_correspondence = dict_results[\"parameter_correspondence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot interesting statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will plot the effect of the noise on the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We choose to plot first the effect of the noise.\n",
    "\n",
    "name_dataset = \"big_2\"\n",
    "\n",
    "# get the interesting data\n",
    "outliers_constant_integer = 3\n",
    "x = dict_parameters_correspondence[\"noise\"]\n",
    "y_kergm = np.mean(kergm_res[:,outliers_constant_integer,:],1)\n",
    "std_kergm = np.std(kergm_res[:,outliers_constant_integer,:],1)\n",
    "y_ipf = np.mean(ipf_res[:,outliers_constant_integer,:],1)\n",
    "std_ipf = np.std(ipf_res[:,outliers_constant_integer,:],1)\n",
    "y_rrwm = np.mean(rrwm_res[:,outliers_constant_integer,:],1)\n",
    "std_rrwm = np.std(rrwm_res[:,outliers_constant_integer,:],1)\n",
    "y_smac = np.mean(smac_res[:,outliers_constant_integer,:],1)\n",
    "std_smac = np.std(smac_res[:,outliers_constant_integer,:],1)\n",
    "\n",
    "\n",
    "# plot the curves\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, y_kergm, label=\"KerGM\", alpha=1)\n",
    "plt.fill_between(x, y_kergm-std_kergm, y_kergm+std_kergm, alpha=0.2)\n",
    "plt.plot(x, y_ipf, label=\"IPF\", alpha=1)\n",
    "plt.fill_between(x, y_ipf-std_ipf, y_ipf+std_ipf, alpha=0.2)\n",
    "plt.plot(x, y_rrwm, label=\"RRWM\", alpha=1)\n",
    "plt.fill_between(x, y_rrwm-std_rrwm, y_rrwm+std_rrwm, alpha=0.2)\n",
    "plt.plot(x, y_smac, label=\"smac\", alpha=1)\n",
    "plt.fill_between(x, y_smac-std_smac, y_smac+std_smac, alpha=0.2)\n",
    "plt.xlabel(\"Noise value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(-0.2,1.1)\n",
    "plt.title(\"Accuracy vs Noise (with \"+str(int(dict_parameters_correspondence[\"outliers\"][outliers_constant_integer]))+\" outliers)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/Accuracy_vs_noise_\"+str(name_dataset))\n",
    "\n",
    "\n",
    "##### Plot the accuracy vs_outlier #####\n",
    "# get the interesting data\n",
    "noise_constant_integer = 3\n",
    "x = dict_parameters_correspondence[\"outliers\"]\n",
    "y_kergm = np.mean(kergm_res[noise_constant_integer,:,:],1)\n",
    "std_kergm = np.std(kergm_res[noise_constant_integer,:,:],1)\n",
    "y_ipf = np.mean(ipf_res[noise_constant_integer,:,:],1)\n",
    "std_ipf = np.std(ipf_res[noise_constant_integer,:,:],1)\n",
    "y_rrwm = np.mean(rrwm_res[noise_constant_integer,:,:],1)\n",
    "std_rrwm = np.std(rrwm_res[noise_constant_integer,:,:],1)\n",
    "y_smac = np.mean(smac_res[noise_constant_integer,:,:],1)\n",
    "std_smac = np.std(smac_res[noise_constant_integer,:,:],1)\n",
    "\n",
    "\n",
    "# plot the curves\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, y_kergm, label=\"KerGM\", alpha=1)\n",
    "plt.fill_between(x, y_kergm-std_kergm, y_kergm+std_kergm, alpha=0.2)\n",
    "plt.plot(x, y_ipf, label=\"IPF\", alpha=1)\n",
    "plt.fill_between(x, y_ipf-std_ipf, y_ipf+std_ipf, alpha=0.2)\n",
    "plt.plot(x, y_rrwm, label=\"RRWM\", alpha=1)\n",
    "plt.fill_between(x, y_rrwm-std_rrwm, y_rrwm+std_rrwm, alpha=0.2)\n",
    "plt.plot(x, y_smac, label=\"smac\", alpha=1)\n",
    "plt.fill_between(x, y_smac-std_smac, y_smac+std_smac, alpha=0.2)\n",
    "plt.xlabel(\"nb_outliers value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.title(\"Accuracy vs Outliers (with noise = \"+str(dict_parameters_correspondence[\"noise\"][noise_constant_integer])+\")\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/Accuracy_vs_outliers_\"+str(name_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_curve(dict_results, name_in_dict, label, x, variable_to_show=\"accuracy\", outliers_constant_integer = None, noise_constant_integer=None, color=None):\n",
    "    \"\"\"\n",
    "    Plot the mean and std of the result of one algorithm\n",
    "    given some constraints on the parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if variable_to_show == \"accuracy\":\n",
    "        place_tuple = 0\n",
    "    elif variable_to_show == \"time\":\n",
    "        place_tuple = 1\n",
    "    \n",
    "    alg_res = dict_results[name_in_dict][place_tuple]\n",
    "    if outliers_constant_integer != None:\n",
    "        y = np.mean(alg_res[:,outliers_constant_integer,:],1)\n",
    "        std = np.std(alg_res[:,outliers_constant_integer,:],1)\n",
    "    elif noise_constant_integer != None:\n",
    "        y = np.mean(alg_res[noise_constant_integer,:,:],1)\n",
    "        std = np.std(alg_res[noise_constant_integer,:,:],1)\n",
    "        \n",
    "    if color == None:\n",
    "        plt.plot(x, y, label=label, alpha=1)\n",
    "        plt.fill_between(x, y-std, y+std, alpha=0.2)\n",
    "    else:\n",
    "        plt.plot(x, y, label=label, alpha=1, color=color)\n",
    "        plt.fill_between(x, y-std, y+std, alpha=0.2, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_dataset = \"simus_complete_F1\"\n",
    "nb_vertices = 85\n",
    "nb_graphs = 25\n",
    "method = \"KerGM\"\n",
    "path_to_data = \"results/simus_complete_F1.pickle\" # change if recall is needed\n",
    "\n",
    "data_to_show = \"F1\"\n",
    "if data_to_show == \"accuracy\":\n",
    "    name_dataset = \"accuracy_\"+name_dataset\n",
    "elif data_to_show ==\"time\":\n",
    "    name_dataset = \"time_\"+name_dataset\n",
    "elif data_to_show ==\"recall\":\n",
    "    name_dataset = \"recall_\"+name_dataset\n",
    "elif data_to_show ==\"prec\":\n",
    "    name_dataset = \"prec_\"+name_dataset\n",
    "elif data_to_show == \"F1\":\n",
    "    name_dataset = \"F1_\" + name_dataset\n",
    "\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "\n",
    "# Affect all the necesary variables\n",
    "dict_parameters_correspondence = dict_results[\"parameter_correspondence\"]\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================ NOISE ===============\")\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "for i in range(len(dict_parameters_correspondence[\"outliers\"])):\n",
    "    \n",
    "    outliers_constant_integer = i\n",
    "    nb_outliers = dict_parameters_correspondence[\"outliers\"][i]\n",
    "    percent_outliers = int(nb_outliers / (nb_vertices + nb_outliers) * 100)\n",
    "    \n",
    "    x = dict_parameters_correspondence[\"noise\"]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    if method == \"KerGM\":\n",
    "        plot_one_curve(dict_results, \"KerGM\", \"KerGM\", x, outliers_constant_integer = outliers_constant_integer)#, variable_to_show=data_to_show)\n",
    "    elif method == \"good_guess\":\n",
    "        plot_one_curve(dict_results, \"good_guess\", \"Good Guess\", x, outliers_constant_integer = outliers_constant_integer)#, variable_to_show=data_to_show)\n",
    "    \n",
    "    #plot_one_curve(dict_results, \n",
    "    #               \"mSync\", \n",
    "    #               \"mSync\", \n",
    "    #               x, \n",
    "    #               outliers_constant_integer = outliers_constant_integer, \n",
    "    #               #variable_to_show=data_to_show,\n",
    "    #               color=\"tab:orange\"\n",
    "    #              )\n",
    "\n",
    "    plot_one_curve(dict_results, \n",
    "                   \"mALS\", \n",
    "                   \"mALS\", \n",
    "                   x, \n",
    "                   outliers_constant_integer = outliers_constant_integer, \n",
    "                   #variable_to_show=data_to_show,\n",
    "                   color = \"tab:red\"\n",
    "                  )\n",
    "\n",
    "    #plot_one_curve(dict_results, \n",
    "    #               \"cao_cst_o\", \n",
    "    #               \"cao_cst_o\", \n",
    "    #               x, \n",
    "    #               outliers_constant_integer = outliers_constant_integer, \n",
    "    #               #variable_to_show=data_to_show,\n",
    "    #               color=\"tab:purple\"\n",
    "    #              )\n",
    "    \n",
    "    plot_one_curve(dict_results, \n",
    "                   \"ipf\", \n",
    "                   \"ipf\", \n",
    "                   x, \n",
    "                   outliers_constant_integer = outliers_constant_integer, \n",
    "                   #variable_to_show=data_to_show,\n",
    "                   #color=\"tab:purple\"\n",
    "                  )\n",
    "    \n",
    "    plot_one_curve(dict_results, \n",
    "                   \"smac\", \n",
    "                   \"smac\", \n",
    "                   x, \n",
    "                   outliers_constant_integer = outliers_constant_integer, \n",
    "                   #variable_to_show=data_to_show,\n",
    "                   #color=\"tab:purple\"\n",
    "                  )\n",
    "    \n",
    "    plot_one_curve(dict_results, \n",
    "                   \"rrwm\", \n",
    "                   \"rrwm\", \n",
    "                   x, \n",
    "                   outliers_constant_integer = outliers_constant_integer, \n",
    "                   #variable_to_show=data_to_show,\n",
    "                   #color=\"tab:purple\"\n",
    "                  )\n",
    "    \n",
    "    #plot_one_curve(dict_results, \n",
    "    #               \"KerGM_2\", \n",
    "    #               \"KerGM_2\", \n",
    "    #               x, \n",
    "    #               outliers_constant_integer = outliers_constant_integer, \n",
    "                   #variable_to_show=data_to_show,\n",
    "                   #color=\"tab:purple\"\n",
    "    #              )\n",
    "    \n",
    "    #plot_one_curve(dict_results, \"cao_cst_s_o\", \"cao_cst_s_o\", x, outliers_constant_integer = outliers_constant_integer, variable_to_show=data_to_show)\n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"Noise variance $\\sigma$\")\n",
    "    if data_to_show == \"accuracy\":\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(-0.1,1.1)\n",
    "    elif data_to_show == \"recall\":\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.ylim(-0.1,1.1)\n",
    "    elif data_to_show == \"prec\":\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.ylim(-0.1,1.1)\n",
    "    elif data_to_show == \"F1\":\n",
    "        plt.ylabel(\"F1-Score\")\n",
    "        plt.ylim(-0.1,1.1)\n",
    "    elif data_to_show == \"time\":\n",
    "        plt.ylabel(\"Time (in seconds)\")\n",
    "        #plt.ylim(-0.2,1.1)\n",
    "    #plt.title(\"Accuracy vs Noise Variance (with {:0d}% outliers, {:0d} graphs, pairwise method {})\".format(percent_outliers, nb_graphs, method))\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18)\n",
    "    plt.xticks(x)\n",
    "    plt.savefig(\"plots/\"+data_to_show+\"_vs_noise_\"+str(name_dataset)+\"_\"+str(int(dict_parameters_correspondence[\"outliers\"][outliers_constant_integer]))+\",\"+str(nb_graphs)+\",\"+method+\".png\")\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"============== OUTLIERS ==============\")\n",
    "print(\"======================================\")\n",
    "\n",
    "#for i in range(len(dict_parameters_correspondence[\"noise\"])):\n",
    "#    noise_constant_integer = i\n",
    "#    x = dict_parameters_correspondence[\"outliers\"]\n",
    "#    x = [int(nb_outliers / (nb_vertices + nb_outliers) * 100) for nb_outliers in x]\n",
    "#    plt.figure(figsize=(10,6))\n",
    "    #plot_one_curve(dict_results, \"KerGM\", \"KerGM\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"mSync\", \"mSync\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"mOpt\", \"mOpt\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao\", \"cao\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_o\", \"cao_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_s_o\", \"cao_s_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_uc\", \"cao_uc\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_uc_o\", \"cao_uc_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_uc_s_o\", \"cao_uc_s_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_pc\", \"cao_pc\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_pc_o\", \"cao_pc_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_pc_s_o\", \"cao_pc_s_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_c\", \"cao_c\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_c_o\", \"cao_c_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    #plot_one_curve(dict_results, \"cao_c_s_o\", \"cao_c_s_o\", x, noise_constant_integer = noise_constant_integer)\n",
    "    \n",
    "    \n",
    "    #plt.xlabel(\"% of outliers\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.ylim(-0.2,1.1)\n",
    "    #plt.title(\"Accuracy vs Outliers (with noise variance = \"+str(dict_parameters_correspondence[\"noise\"][noise_constant_integer])+\")\")\n",
    "    #plt.legend()\n",
    "    #plt.savefig(\"plots/Accuracy_vs_outliers_\"+str(name_dataset)+\"_\"+str(dict_parameters_correspondence[\"noise\"][noise_constant_integer])+\".png\" )\n",
    "    #plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots results from mixed pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_curve_mix(dict_results, name_in_dict, label, x, variable_to_show=\"accuracy\", outliers_constant_integer = 0, noise_constant_integer=0, color=None):\n",
    "    \"\"\"\n",
    "    Plot the mean and std of the result of one algorithm - changed for mixed pairwise\n",
    "    given some constraints on the parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if variable_to_show == \"accuracy\":\n",
    "        place_tuple = 0\n",
    "        type_data = \"accuracy\"\n",
    "    elif variable_to_show == \"distance\":\n",
    "        place_tuple = 0\n",
    "        type_data = \"distance_to_original\"\n",
    "        \n",
    "    \n",
    "    list_res_y = []\n",
    "    list_res_std = []\n",
    "    for elem in x:\n",
    "        alg_res = dict_results[elem][type_data][name_in_dict][place_tuple]\n",
    "        list_res_y.append(np.mean(alg_res[noise_constant_integer,outliers_constant_integer,:]))\n",
    "        list_res_std.append(np.std(alg_res[noise_constant_integer,outliers_constant_integer,:]))\n",
    "         \n",
    "    list_res_y = np.array(list_res_y)\n",
    "    list_res_std = np.array(list_res_std)\n",
    "    if color == None:\n",
    "        plt.plot(x, list_res_y, label=label, alpha=1)\n",
    "        plt.fill_between(x, list_res_y-list_res_std, list_res_y+list_res_std, alpha=0.2)\n",
    "    else:\n",
    "        plt.plot(x, list_res_y, label=label, alpha=1, color=color)\n",
    "        plt.fill_between(x, list_res_y-list_res_std, list_res_y+list_res_std, alpha=0.2, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define some variables for the plot\n",
    "name_dataset = \"big_high_noise_prec\"\n",
    "nb_vertices = 85\n",
    "nb_graphs = 25\n",
    "method = \"mix\"\n",
    "folder_res = method + \"_\" + name_dataset\n",
    "folder_path = os.path.join(\"plots\",folder_res)\n",
    "path_to_data = \"results/big_high_noise_mix_prec.pickle\"\n",
    "\n",
    "data_to_show = \"accuracy\"\n",
    "if data_to_show == \"accuracy\":\n",
    "    name_dataset = \"accuracy_\"+name_dataset\n",
    "    name_to_show = \"Accuracy\"\n",
    "elif data_to_show ==\"distance\":\n",
    "    name_dataset = \"distance_\"+name_dataset\n",
    "    name_to_show = \"Distance to KerGM\"\n",
    "\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "\n",
    "# Create the plot sub folder if it does not exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "\n",
    "# Affect all the necesary variables\n",
    "dict_parameters_correspondence = dict_results[\"parameter_correspondence\"]\n",
    "del dict_results[\"parameter_correspondence\"]\n",
    "x = list(dict_results.keys())\n",
    "x.sort()\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================ NOISE ===============\")\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "for i_outliers in range(len(dict_parameters_correspondence[\"outliers\"])):\n",
    "    for j_noise in range(len(dict_parameters_correspondence[\"noise\"])):\n",
    "        \n",
    "        outliers_constant_integer = i_outliers\n",
    "        noise_constant_integer = j_noise\n",
    "        nb_outliers = dict_parameters_correspondence[\"outliers\"][i_outliers]\n",
    "        percent_outliers = int(nb_outliers / (nb_vertices + nb_outliers) * 100)\n",
    "    \n",
    "        plt.figure(figsize=(10,6))\n",
    "        plot_one_curve_mix(dict_results, \n",
    "                           \"mSync\", \n",
    "                           \"mSync\", \n",
    "                           x, \n",
    "                           data_to_show, \n",
    "                           outliers_constant_integer, \n",
    "                           noise_constant_integer,\n",
    "                           \"tab:orange\"\n",
    "                          )\n",
    "        plot_one_curve_mix(dict_results, \n",
    "                           \"mALS\", \n",
    "                           \"mALS\", \n",
    "                           x, \n",
    "                           data_to_show, \n",
    "                           outliers_constant_integer, \n",
    "                           noise_constant_integer,\n",
    "                           \"tab:red\"\n",
    "                          )\n",
    "        plot_one_curve_mix(dict_results, \n",
    "                           \"cao_cst_o\", \n",
    "                           \"cao_cst_o\", \n",
    "                           x, \n",
    "                           data_to_show, \n",
    "                           outliers_constant_integer, \n",
    "                           noise_constant_integer,\n",
    "                           \"tab:purple\"\n",
    "                          )\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        plt.xlabel(\"% Of matching done with good guess\")\n",
    "        if data_to_show == \"accuracy\":\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.ylim(-0.1,1.1)\n",
    "        elif data_to_show == \"distance\":\n",
    "            plt.ylabel(\"Difference with the results using full KerGM\")\n",
    "            plt.ylim(-0.1,1.1)\n",
    "        plt.title(name_to_show + \" vs Mix Rate (with {:0d}% outliers, {:0d} Noise variance, {:0d} graphs, pairwise method {})\".format(percent_outliers,\n",
    "                                                                                                                                      int(dict_parameters_correspondence[\"noise\"][noise_constant_integer]), \n",
    "                                                                                                                                      nb_graphs, \n",
    "                                                                                                                                      method))\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(folder_path,data_to_show+\"_vs_mix_\"+str(name_dataset)+\"_\"+str(int(dict_parameters_correspondence[\"outliers\"][outliers_constant_integer]))+\",\"+str(int(dict_parameters_correspondence[\"noise\"][noise_constant_integer]))+\",\"+str(nb_graphs)+\",\"+method+\".png\"))\n",
    "        plt.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics when using different numbers of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_curve_subgraphs(dict_results, name_in_dict, label, x, variable_to_show=\"accuracy\", outliers_constant_integer = 0, noise_constant_integer=0, color=None):\n",
    "    \"\"\"\n",
    "    Plot the mean and std of the result of one algorithm - changed for different numbers of subgraphs\n",
    "    given some constraints on the parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if variable_to_show == \"accuracy\":\n",
    "        place_tuple = 0\n",
    "    elif variable_to_show == \"time\":\n",
    "        place_tuple = 1\n",
    "        \n",
    "    list_res_y = []\n",
    "    list_res_std = []\n",
    "    for elem in x:\n",
    "        #print(elem, place_tuple, name_in_dict)\n",
    "        #alg_res = dict_results[elem][name_in_dict]\n",
    "        alg_res = dict_results[elem][name_in_dict][place_tuple]\n",
    "        list_res_y.append(np.mean(alg_res[noise_constant_integer,outliers_constant_integer,:]))\n",
    "        list_res_std.append(np.std(alg_res[noise_constant_integer,outliers_constant_integer,:]))\n",
    "         \n",
    "    list_res_y = np.array(list_res_y)\n",
    "    list_res_std = np.array(list_res_std)\n",
    "    if color == None:\n",
    "        plt.plot(x, list_res_y, label=label, alpha=1)\n",
    "        plt.fill_between(x, list_res_y-list_res_std, list_res_y+list_res_std, alpha=0.2)\n",
    "    else:\n",
    "        plt.plot(x, list_res_y, label=label, alpha=1, color=color)\n",
    "        plt.fill_between(x, list_res_y-list_res_std, list_res_y+list_res_std, alpha=0.2, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define some variables for the plot\n",
    "name_dataset = \"big_high_noise_subgraphs\"\n",
    "nb_vertices = 85\n",
    "#nb_graphs = 10\n",
    "method = \"KerGM\"\n",
    "folder_res = \"sub_graphs\" + \"_\" + name_dataset\n",
    "folder_path = os.path.join(\"plots\",folder_res)\n",
    "path_to_data = \"results/big_high_noise_sub_graphs.pickle\"\n",
    "\n",
    "data_to_show = \"accuracy\"\n",
    "if data_to_show == \"accuracy\":\n",
    "    name_dataset = \"accuracy_\"+name_dataset\n",
    "    name_to_show = \"Accuracy\"\n",
    "elif data_to_show ==\"time\":\n",
    "    name_dataset = \"time_\"+name_dataset\n",
    "    name_to_show = \"Time\"\n",
    "\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "\n",
    "# Create the plot sub folder if it does not exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "\n",
    "# Affect all the necesary variables\n",
    "dict_parameters_correspondence = dict_results[\"parameter_correspondence\"]\n",
    "del dict_results[\"parameter_correspondence\"]\n",
    "x = list(dict_results.keys())\n",
    "x.sort()\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================ NOISE ===============\")\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "for i_outliers in range(len(dict_parameters_correspondence[\"outliers\"])):\n",
    "    for j_noise in range(len(dict_parameters_correspondence[\"noise\"])):\n",
    "        \n",
    "        outliers_constant_integer = i_outliers\n",
    "        noise_constant_integer = j_noise\n",
    "        nb_outliers = dict_parameters_correspondence[\"outliers\"][i_outliers]\n",
    "        percent_outliers = int(nb_outliers / (nb_vertices + nb_outliers) * 100)\n",
    "    \n",
    "        plt.figure(figsize=(10,6))\n",
    "        plot_one_curve_subgraphs(dict_results, \n",
    "                           \"mSync\", \n",
    "                           \"mSync\", \n",
    "                           x, \n",
    "                           data_to_show, \n",
    "                           outliers_constant_integer, \n",
    "                           noise_constant_integer,\n",
    "                           \"tab:orange\"\n",
    "                          )\n",
    "        plot_one_curve_subgraphs(dict_results, \n",
    "                           \"mALS\", \n",
    "                           \"mALS\", \n",
    "                           x, \n",
    "                           data_to_show, \n",
    "                           outliers_constant_integer, \n",
    "                           noise_constant_integer,\n",
    "                           \"tab:red\"\n",
    "                          )\n",
    "        plot_one_curve_subgraphs(dict_results, \n",
    "                           \"cao_cst_o\", \n",
    "                           \"cao_cst_o\", \n",
    "                           x, \n",
    "                           data_to_show, \n",
    "                           outliers_constant_integer, \n",
    "                           noise_constant_integer,\n",
    "                           \"tab:purple\"\n",
    "                          )\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        plt.xlabel(\"Number of graphs used\")\n",
    "        if data_to_show == \"accuracy\":\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.ylim(-0.1,1.1)\n",
    "        elif data_to_show == \"time\":\n",
    "            plt.ylabel(\"Time\")\n",
    "            #plt.ylim(-0.1,1.1)\n",
    "        plt.title(name_to_show + \" vs nb of graphs (with {:0d}% outliers, {:0d} Noise variance, pairwise method {})\".format(percent_outliers,\n",
    "                                                                                                                                      int(dict_parameters_correspondence[\"noise\"][noise_constant_integer]), \n",
    "                                                                                                                                      method))\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(folder_path,data_to_show+\"_vs_graphs_\"+str(name_dataset)+\"_\"+str(int(dict_parameters_correspondence[\"outliers\"][outliers_constant_integer]))+\",\"+str(int(dict_parameters_correspondence[\"noise\"][noise_constant_integer]))+\",\"+method+\".png\"))\n",
    "        plt.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot labeling results on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "path_to_data = \"results/des_gros_test_label.pickle\"\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_curve(dict_parameters, x, result_tensor, label, outliers_constant_integer = None, noise_constant_integer=None):\n",
    "    \"\"\"\n",
    "    Plot the accuracy result by fixing either noise or outliers at one level\n",
    "    \"\"\"\n",
    "    \n",
    "    if outliers_constant_integer is not None:\n",
    "        matrix_to_use = result_tensor[:,outliers_constant_integer,:]\n",
    "    else:\n",
    "        matrix_to_use = result_tensor[noise_constant_integer,:,:]\n",
    "        \n",
    "    y_mean = matrix_to_use.mean(1)\n",
    "    print(y_mean)\n",
    "    y_std = matrix_to_use.std(1)\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(x, y_mean, label=label)\n",
    "    plt.fill_between(x, y_mean - y_std, y_mean + y_std, alpha=0.2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define some variables for the plot\n",
    "name_dataset = \"big_high_noise_labelling_dbscan_precision\"\n",
    "nb_vertices = 85\n",
    "nb_graphs = 25\n",
    "folder_res = name_dataset\n",
    "folder_path = os.path.join(\"plots\",folder_res)\n",
    "\n",
    "# load mALS basline accuracy\n",
    "path_to_baseline = \"results/big_high_noise_prec.pickle\"\n",
    "pickle_in = open(path_to_baseline,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "result_baseline_mALS = dict_results[\"mALS\"][0]\n",
    "\n",
    "# load the labelling from dbscan\n",
    "path_to_data = \"results/big_high_noise_clustering_precision.pickle\"\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "\n",
    "# Create the plot sub folder if it does not exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "\n",
    "# Affect all the necesary variables\n",
    "dict_parameters_correspondence = dict_results[\"parameters_correspondence\"]\n",
    "result_tensor = dict_results[\"mALS\"]\n",
    "#del dict_results[\"parameter_correspondence\"]\n",
    "\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================ NOISE ===============\")\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "for i_outliers in range(len(dict_parameters_correspondence[\"outliers\"])):\n",
    "\n",
    "        \n",
    "    outliers_constant_integer = i_outliers\n",
    "    nb_outliers = dict_parameters_correspondence[\"outliers\"][i_outliers]\n",
    "    percent_outliers = int(nb_outliers / (nb_vertices + nb_outliers) * 100)\n",
    "    x = dict_parameters_correspondence[\"noise\"]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    # plot mALS baseline\n",
    "    plot_one_curve(dict_parameters_correspondence,\n",
    "                  x,\n",
    "                  result_baseline_mALS,\n",
    "                  label=\"mALS\",\n",
    "                  outliers_constant_integer=outliers_constant_integer,\n",
    "                  )\n",
    "    \n",
    "    plot_one_curve(dict_parameters_correspondence, \n",
    "                   x,\n",
    "                   result_tensor,\n",
    "                   label=\"DBSCAN\",\n",
    "                   outliers_constant_integer=outliers_constant_integer,\n",
    "                  )\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"Noise variance\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim(-0.1,1.1)\n",
    "        \n",
    "    plt.title(\"Precision vs Noise Variance (with {:0d}% outliers,)\".format(percent_outliers))\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(folder_path,\"Precision\"+\"_vs_noise_\"+str(dict_parameters_correspondence[\"outliers\"][outliers_constant_integer])+\".png\"))\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot effect of parameters on labelling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_curve(dict_parameters, x, result_tensor, label, outliers_constant_integer = None, noise_constant_integer=None):\n",
    "    \"\"\"\n",
    "    Plot the accuracy result by fixing either noise or outliers at one level\n",
    "    \"\"\"\n",
    "    \n",
    "    if outliers_constant_integer is not None:\n",
    "        matrix_to_use = result_tensor[:,outliers_constant_integer,:]\n",
    "    else:\n",
    "        matrix_to_use = result_tensor[noise_constant_integer,:,:]\n",
    "        \n",
    "    y_mean = matrix_to_use.mean(1)\n",
    "    print(y_mean)\n",
    "    y_std = matrix_to_use.std(1)\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(x, y_mean, label=label)\n",
    "    plt.fill_between(x, y_mean - y_std, y_mean + y_std, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define some variables for the plot\n",
    "name_dataset = \"big_high_noise_labelling_dbscan_params_prec\"\n",
    "nb_vertices = 85\n",
    "nb_graphs = 25\n",
    "folder_res = name_dataset\n",
    "folder_path = os.path.join(\"plots\",folder_res)\n",
    "\n",
    "# load mALS basline accuracy\n",
    "path_to_baseline = \"results/big_high_noise.pickle\"\n",
    "pickle_in = open(path_to_baseline,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "result_baseline_mALS = dict_results[\"mALS\"][0]\n",
    "\n",
    "# load the labelling from dbscan\n",
    "path_to_data = \"results/big_high_noise_clustering_params_prec.pickle\"\n",
    "pickle_in = open(path_to_data,\"rb\")\n",
    "dict_results = pickle.load(pickle_in)\n",
    "\n",
    "# Create the plot sub folder if it does not exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "\n",
    "# Affect all the necesary variables\n",
    "dict_parameters_correspondence = dict_results[\"parameters_correspondence\"]\n",
    "result_tensor = dict_results[\"DBSCAN\"]\n",
    "#del dict_results[\"parameter_correspondence\"]\n",
    "\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================ NOISE ===============\")\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "for i_outliers in range(len(dict_parameters_correspondence[\"outliers\"])):\n",
    "\n",
    "        \n",
    "    outliers_constant_integer = i_outliers\n",
    "    nb_outliers = dict_parameters_correspondence[\"outliers\"][i_outliers]\n",
    "    percent_outliers = int(nb_outliers / (nb_vertices + nb_outliers) * 100)\n",
    "    x = dict_parameters_correspondence[\"noise\"]\n",
    "    \n",
    "    for i_minsamp in range(1,len(dict_parameters_correspondence[\"minsamp\"])):\n",
    "        \n",
    "        plt.figure(figsize=(10,6))\n",
    "    \n",
    "        # plot mALS baseline\n",
    "        plot_one_curve(dict_parameters_correspondence,\n",
    "                      x,\n",
    "                      result_baseline_mALS,\n",
    "                      label=\"mALS\",\n",
    "                      outliers_constant_integer=outliers_constant_integer,\n",
    "                      )\n",
    "    \n",
    "        for i_eps in range(1,len(dict_parameters_correspondence[\"eps\"]),2):\n",
    "            # plot one curve dbscan\n",
    "    \n",
    "            eps_val = dict_parameters_correspondence[\"eps\"][i_eps]\n",
    "            minsamp_val = dict_parameters_correspondence[\"minsamp\"][i_minsamp]\n",
    "    \n",
    "            plot_one_curve(dict_parameters_correspondence, \n",
    "                       x,\n",
    "                       result_tensor[:,:,i_eps,i_minsamp,:],\n",
    "                       label=\"DBSCAN eps=\"+str(eps_val)+\" min_samp=\"+str(minsamp_val),\n",
    "                       outliers_constant_integer=outliers_constant_integer,\n",
    "                      )\n",
    "        \n",
    "            #plot_one_curve(dict_parameters_correspondence, \n",
    "            #           x,\n",
    "            #           result_tensor[:,:,5,10,:],\n",
    "            #           label=\"DBSCAN eps=\"+str(5)+\" min_samp=\"+str(10),\n",
    "            #           outliers_constant_integer=outliers_constant_integer,\n",
    "            #          )\n",
    "    \n",
    "    \n",
    "    \n",
    "        plt.xlabel(\"Noise variance\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.ylim(-0.1,1.1)\n",
    "        \n",
    "        plt.title(\"Precision vs Noise Variance (with {:0d}% outliers,)\".format(percent_outliers))\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(folder_path,\"Precision\"+\"_vs_noise_\"+str(dict_parameters_correspondence[\"outliers\"][outliers_constant_integer])+\"_eps_\"+str(i_eps)+\"_minsamp_\"+str(i_minsamp)+\".png\"))\n",
    "        plt.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "104*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create plots from different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"results/big_low_noise.pickle\",\"rb\")\n",
    "dict_result_2 = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"results/big_high_noise.pickle\",\"rb\")\n",
    "dict_result_3 = pickle.load(pickle_in)\n",
    "print(dict_result_2['parameter_correspondence'])\n",
    "\n",
    "dict_list = [dict_result_2, dict_result_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_different_experiments(list_dict_result, x_param=\"noise\", static_param_value=0, name_dataset=\"full\"):\n",
    "    \"\"\" Get the result of different experiments and combine\n",
    "        them to create a plot for one parameter throughout the experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    if x_param == \"noise\":\n",
    "        static_param = \"outliers\"\n",
    "    else:\n",
    "        static_param = \"noise\"\n",
    "    \n",
    "    name_algorithms = [\"KerGM\",\"mALS\",\"mSync\",\"cao_cst_o\"]\n",
    "    label_corres = {\"kergm\":\"KerGM\", \"ipf\":\"IPF\", \"rrwm\":\"RRWM\", \"smac\":\"Smac\"}\n",
    "    \n",
    "    # create three lists x, y and err for one plot\n",
    "    x_final = []\n",
    "    y_final = {alg_name:{} for alg_name in name_algorithms}\n",
    "    err_final = {alg_name:{} for alg_name in name_algorithms}\n",
    "    \n",
    "    #y_tuple = {alg_name:{} for alg_name in name_algorithms}\n",
    "    #err_tuple = {alg_name:{} for alg_name in name_algorithms}\n",
    "    \n",
    "    \n",
    "    # We go through all the dict_results and we keep the tuple (x,val)\n",
    "    for dict_result in list_dict_result:\n",
    "        \n",
    "        # We load the data\n",
    "        dict_parameters_correspondence = dict_result[\"parameter_correspondence\"]\n",
    "        kergm_res = dict_result[\"KerGM\"]\n",
    "        ipf_res = dict_result[\"mALS\"]\n",
    "        rrwm_res = dict_result[\"mSync\"]\n",
    "        smac_res = dict_result[\"cao_cst_o\"]\n",
    "        \n",
    "        algos_res = [(name, dict_result[name]) for name in name_algorithms]\n",
    "\n",
    "        # We get the integer value for the static param\n",
    "        static_param_integer = dict_parameters_correspondence[static_param].index(static_param_value)\n",
    "        \n",
    "        # We fill the result dictionaries\n",
    "        for variable_param_i, variable_param_val in enumerate(dict_parameters_correspondence[x_param]):\n",
    "            \n",
    "            for name_alg, dict_res_alg in algos_res:\n",
    "                \n",
    "                # We create the entry for the given x if it does not exist :\n",
    "                if variable_param_val not in y_final[name_alg]:\n",
    "                    y_final[name_alg][variable_param_val] = []\n",
    "                    \n",
    "                if variable_param_val not in err_final[name_alg]:\n",
    "                    err_final[name_alg][variable_param_val] = []\n",
    "                    \n",
    "                # We compute the result\n",
    "                if x_param == \"noise\":\n",
    "                    #print(variable_param_i, static_param_integer, dict_res_alg[variable_param_i,static_param_integer,:])\n",
    "                    #print(variable_param_i, static_param_integer, dict_res_alg)\n",
    "                    #y_tmp = np.mean(dict_res_alg[static_param_integer,variable_param_i,:],1)\n",
    "                    #err_tmp = np.std(dict_res_alg[static_param_integer,variable_param_i,:],1)\n",
    "                    y_tmp = np.mean(dict_res_alg[variable_param_i,static_param_integer,:])\n",
    "                    err_tmp = np.std(dict_res_alg[variable_param_i,static_param_integer,:])\n",
    "                else:\n",
    "                    y_tmp = np.mean(dict_res_alg[static_param_integer,variable_param_i,:])\n",
    "                    err_tmp = np.std(dict_res_alg[static_param_integer,variable_param_i,:])\n",
    "                    \n",
    "                y_final[name_alg][variable_param_val].append(y_tmp)\n",
    "                err_final[name_alg][variable_param_val].append(err_tmp)\n",
    "                \n",
    "    # Now that we have the numbers we take the mean of the points where there are several values\n",
    "    for alg_name in y_final:\n",
    "        for x_val in y_final[alg_name]:\n",
    "            y_final[alg_name][x_val] = np.mean(y_final[alg_name][x_val])\n",
    "            err_final[alg_name][x_val] = np.mean(err_final[alg_name][x_val])\n",
    "            \n",
    "            \n",
    "    # We plot everything of interest   \n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    for alg_name in y_final:\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        err_list = []\n",
    "        \n",
    "        for x_val in y_final[alg_name]:\n",
    "            x_list.append(x_val)\n",
    "            y_list.append(y_final[alg_name][x_val])\n",
    "            err_list.append(err_final[alg_name][x_val])\n",
    "            \n",
    "        y_list = np.array(y_list)\n",
    "        err_list = np.array(err_list)\n",
    "        plt.plot(x_list, y_list, label=label_corres[alg_name])\n",
    "        plt.fill_between(x_list, y_list-err_list, y_list+err_list, alpha=0.2)\n",
    "       \n",
    "    if x_param == \"noise\":\n",
    "        percent_outliers = int(static_param_value/(90 + static_param_value) * 100)\n",
    "        plt.xlabel(\"Noise variance\")\n",
    "        plt.title(\"Accuracy vs Noise Variance (with {:0d}% outliers)\".format(percent_outliers))\n",
    "        name_to_save = \"plots/Accuracy_vs_noise_\"+name_dataset+\"_\"+str(static_param_value)+\".png\"\n",
    "\n",
    "    else:\n",
    "        x_list = [int(x/(90 + x) * 100) for x in x_list]\n",
    "        plt.xlabel(\"% of outliers\")\n",
    "        plt.title(\"Accuracy vs Outliers (with noise variance = \"+str(static_param_value)+\")\")\n",
    "        name_to_save = \"plots/Accuracy_vs_outliers_\"+name_dataset+\"_\"+str(static_param_value)+\".png\"\n",
    "    \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(-0.2,1.1)\n",
    "    plt.legend()\n",
    "    plt.savefig(name_to_save)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return y_final, err_final\n",
    "                        \n",
    "a, b = plot_from_different_experiments([dict_results],\"outliers\",20)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = plot_from_different_experiments(dict_list,\"noise\",25)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.linspace(0,1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "[i/num for i in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/rohit/PhD_Work/stage_nathan/data/simu_graph/noise_1000,outliers_0/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_1 = pickle.load(open(os.path.join(path,\"graphs\",\"graph_\"+str(0)+\".gpickle\"),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3,1,7,5,]\n",
    "b = ['l','m','n','o']\n",
    "c = ['p','q','r','s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(a, b, c)\n",
    "zipped = sorted(zipped,reverse = True)\n",
    "zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = []\n",
    "a2 = []\n",
    "for l,m,n in zipped:\n",
    "    print(l,m,n)\n",
    "    a1.append(m)\n",
    "    a2.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[19][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameters_name in os.listdir(path):\n",
    "    for run_name in os.listdir(os.path.join(path,parameters_name)):\n",
    "        path_to_run = os.path.join(path,parameters_name,run_name)\n",
    "        print(path_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_graphs = os.path.join(path_to_run,\"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g = nx.read_gpickle(os.path.join(path_to_graphs, \"graph_0.gpickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
